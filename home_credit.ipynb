{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e21b22d0fe4d87",
   "metadata": {},
   "source": [
    "<h1><center>Home Credit Risk Prediction</center></h1>\n",
    "<center>November 2024</center>\n",
    "<center>Celine Ng</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c75c1570ece7ec",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. Project Introduction   \n",
    "    1. Notebook Preparation\n",
    "    1. Data loading\n",
    "1. Main Data Preparation\n",
    "    1. Data cleaning\n",
    "    1. Dataframes and keys\n",
    "    1. Main data preparation\n",
    "1. Initial Data Cleaning\n",
    "    1. Duplicate rows\n",
    "    1. Datatypes\n",
    "    1. Missing values\n",
    "    1. Values\n",
    "1. EDA\n",
    "    1. Correlation\n",
    "    1. Statistical Inference\n",
    "    1. Distribution\n",
    "1. Data Preprocessing\n",
    "1. Feature Selection\n",
    "    1. All features included\n",
    "    1. Mutual Information\n",
    "    1. PCA\n",
    "1. Models\n",
    "    1. Baseline model\n",
    "    1. Basic model\n",
    "    1. Hyperparameter Tuning\n",
    "    1. Test Data\n",
    "    1. Final Model\n",
    "    1. Deployment\n",
    "    1. Model Interpretation\n",
    "1. Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b3f336b582d550",
   "metadata": {},
   "source": [
    "# 1. Project Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69285ceecfe76584",
   "metadata": {},
   "source": [
    "**Crucial problem for retail banks** <br>\n",
    "1. Minimize loan defaults by evaluating credit risk accurately.\n",
    "2. Maximize profits by better identifying customers that are NOT \n",
    "currently handed out loans but are potentially reliable.\n",
    "\n",
    "\n",
    "**Project Objective**<br>\n",
    "The second problem is not solvable with provided data. So the focus of this \n",
    "project would be the following:<br>\n",
    "1. Improve risk evaluation accuracy to retail banks. In practice\n",
    " meaning target variable classification.\n",
    "2. Evaluate feature importance to explain decisions.\n",
    "3. Provide actionable insights to improve credit scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373435036ab7d71f",
   "metadata": {},
   "source": [
    "**Initial Plan**<br>\n",
    "1. Data cleaning (missing values counting) of each table\n",
    "2. Select important tables\n",
    "3. Aggregate table information - 1 row per person instead of loan\n",
    "4. Join columns to main table\n",
    "5. EDA and statistical inference\n",
    "6. Feature engineering - New feature creations with domain knowledge\n",
    "7. Feature selection\n",
    "8. Modeling \n",
    "9. Hyperparameter tuning\n",
    "10. Evaluation with cross validation\n",
    "11. Deployment\n",
    "12. Model interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c69e43e1f7152",
   "metadata": {},
   "source": [
    "## 1.1. Notebook Preparation"
   ]
  },
  {
   "cell_type": "code",
   "id": "f11e9fd1f4500c72",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "%%capture\n",
    "%pip install -r requirements.txt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ad6279e37a232625",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "from utils.eda import *\n",
    "from utils.model import *\n",
    "from utils.stats import *\n",
    "from utils.data_preparation import *\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7f2b526f4c25fccb",
   "metadata": {},
   "source": [
    "## 1.2. Data Loading\n",
    "\n",
    "Objective: Brief overview of our datasets, including the features and target \n",
    "variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc2d132301cccf3",
   "metadata": {},
   "source": [
    "The data comes with 10 separate CSV files. It is originally based on a Kaggle\n",
    " competition that is now closed, \n",
    "[Home Credit Default Risk](https://www.kaggle.com/competitions/home-credit-default-risk/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ea2f62d73cea3",
   "metadata": {},
   "source": [
    "<ol>\n",
    "<li>application_{train|test}.csv\n",
    "\n",
    "This is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).\n",
    "Static data for all applications. One row represents one loan in our data \n",
    "sample.</li>\n",
    "<li>bureau.csv\n",
    "\n",
    "All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample).\n",
    "For every loan in our sample, there are as many rows as number of credits \n",
    "the client had in Credit Bureau before the application date.</li>\n",
    "<li>bureau_balance.csv\n",
    "\n",
    "Monthly balances of previous credits in Credit Bureau.\n",
    "This table has one row for each month of history of every previous credit \n",
    "reported to Credit Bureau – i.e the table has (#loans in sample * # of \n",
    "relative previous credits * # of months where we have some history \n",
    "observable for the previous credits) rows.</li>\n",
    "<li>POS_CASH_balance.csv\n",
    "\n",
    "Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.\n",
    "This table has one row for each month of history of every previous credit in\n",
    " Home Credit (consumer credit and cash loans) related to loans in our sample\n",
    "  – i.e. the table has (#loans in sample * # of relative previous credits \n",
    "  *# of months in which we have some history observable for the previous \n",
    "  credits) rows.</li>\n",
    "<li>credit_card_balance.csv\n",
    "\n",
    "Monthly balance snapshots of previous credit cards that the applicant has with Home Credit.\n",
    "This table has one row for each month of history of every previous credit in\n",
    " Home Credit (consumer credit and cash loans) related to loans in our sample\n",
    "  – i.e. the table has (#loans in sample * # of relative previous credit \n",
    "  cards *# of months where we have some history observable for the previous\n",
    "   credit card) rows.</li>\n",
    "<li>previous_application.csv\n",
    "\n",
    "All previous applications for Home Credit loans of clients who have loans in our sample.\n",
    "There is one row for each previous application related to loans in our data \n",
    "sample.</li>\n",
    "<li>installments_payments.csv\n",
    "\n",
    "Repayment history for the previously disbursed credits in Home Credit related to the loans in our sample.\n",
    "There is a) one row for every payment that was made plus b) one row each for missed payment.\n",
    "One row is equivalent to one payment of one installment OR one installment \n",
    "corresponding to one payment of one previous Home Credit credit related to \n",
    "loans in our sample.</li>\n",
    "\n",
    "**Adjacent csv files:**\n",
    "<li>HomeCredit_columns_description.csv\n",
    "\n",
    "This file contains descriptions for the columns in the various data files.</li>\n",
    "\n",
    "<li>sample submission.csv\n",
    "Sample submission file for Kaggle competition</li>\n",
    "</ol>\n",
    "\n",
    "*Non-adjacent csv files will be converted into pkl files for more efficient \n",
    "memory usage. To do so, please run 'convert_csv_to_pkl.py'.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e5a1b32f3785f0",
   "metadata": {},
   "source": "**Home Credit columns description**"
  },
  {
   "cell_type": "code",
   "id": "abacc925a60f2d89",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "description = pd.read_csv('data_csv/HomeCredit_columns_description.csv', encoding='latin1')\n",
    "display(description.head())\n",
    "description_shape = description.shape\n",
    "print(f\"Number of rows on bureau data_csv: {description_shape[0]}\\nNumber of \"\n",
    "      f\"columns on bureau data_csv: {description_shape[1]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1f9cb1a441006690",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "description.iloc[1, 3]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "801e8c64bdfbfa45",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "description.loc[description['Row'] == 'SK_ID_CURR', 'Description']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "750ffe56f62c3154",
   "metadata": {},
   "source": "**Application train**"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "application_train = pd.read_pickle('data_pkl/application_train.pkl')\n",
    "display(application_train.head())\n",
    "application_train_shape = application_train.shape\n",
    "print(f\"Number of rows on train data_csv: {application_train_shape[0]}\\nNumber of \"\n",
    "      f\"columns on train data_csv: {application_train_shape[1]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Application Test**",
   "id": "34d39419cb29bbee"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "application_test = pd.read_pickle('data_pkl/application_test.pkl')\n",
    "display(application_test.head())\n",
    "application_test_shape = application_test.shape\n",
    "print(f\"Number of rows on train data_csv: {application_test_shape[0]}\\nNumber of \"\n",
    "      f\"columns on train data_csv: {application_test_shape[1]}\")"
   ],
   "id": "95d98f3c651871e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For both application train and test, SK_ID_CURR is the key that identifies \n",
    "each row in the table."
   ],
   "id": "36ff668b6ba87e20"
  },
  {
   "cell_type": "markdown",
   "id": "569f6e61bfc2fa1b",
   "metadata": {},
   "source": "**Bureau**"
  },
  {
   "cell_type": "code",
   "id": "8ad393706ba67846",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "bureau = pd.read_pickle('data_pkl/bureau.pkl')\n",
    "display(bureau.head())\n",
    "bureau_shape = bureau.shape\n",
    "print(f\"Number of rows on bureau data_csv: {bureau_shape[0]}\\nNumber of \"\n",
    "      f\"columns on bureau data_csv: {bureau_shape[1]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ed23005a292c1ac8",
   "metadata": {},
   "source": "**Bureau Balance**"
  },
  {
   "cell_type": "code",
   "id": "60ecddd6ca385d2f",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "bureau_balance = pd.read_pickle('data_pkl/bureau_balance.pkl')\n",
    "display(bureau_balance.head())\n",
    "bureau_balance_shape = bureau_balance.shape\n",
    "print(f\"Number of rows on bureau data_csv: {bureau_balance_shape[0]}\\nNumber of \"\n",
    "      f\"columns on bureau data_csv: {bureau_balance_shape[1]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43b844c80adb0fb5",
   "metadata": {},
   "source": "**Previous Application**"
  },
  {
   "cell_type": "code",
   "id": "c594ef6f06ae0f75",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "previous_application = pd.read_pickle('data_pkl/previous_application.pkl')\n",
    "display(previous_application.head())\n",
    "previous_application_shape = previous_application.shape\n",
    "print(f\"Number of rows on bureau data_csv: {previous_application_shape[0]}\\nNumber of \"\n",
    "      f\"columns on bureau data_csv: {previous_application_shape[1]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ad437f82bef6786d",
   "metadata": {},
   "source": "**POS CASH balance**"
  },
  {
   "cell_type": "code",
   "id": "8888ce27e107ad4a",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "POS_CASH_balance = pd.read_pickle('data_pkl/POS_CASH_balance.pkl')\n",
    "display(POS_CASH_balance.head())\n",
    "POS_CASH_balance_shape = POS_CASH_balance.shape\n",
    "print(f\"Number of rows on bureau data_csv: {POS_CASH_balance_shape[0]}\\nNumber of \"\n",
    "      f\"columns on bureau data_csv: {POS_CASH_balance_shape[1]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "737e50e2acc6b027",
   "metadata": {},
   "source": "**Installments Payments**"
  },
  {
   "cell_type": "code",
   "id": "530a9961193589b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "installments_payments = pd.read_pickle('data_pkl/installments_payments.pkl')\n",
    "display(installments_payments.head())\n",
    "installments_payments_shape = installments_payments.shape\n",
    "print(f\"Number of rows on bureau data_csv: {installments_payments_shape[0]}\\nNumber of \"\n",
    "      f\"columns on bureau data_csv: {installments_payments_shape[1]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "34efdee1ab125b30",
   "metadata": {},
   "source": "**Credit Card Balance**"
  },
  {
   "cell_type": "code",
   "id": "e837debc553a5011",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "credit_card_balance = pd.read_pickle('data_pkl/credit_card_balance.pkl')\n",
    "display(credit_card_balance.head())\n",
    "credit_card_balance_shape = credit_card_balance.shape\n",
    "print(f\"Number of rows on bureau data_csv: {credit_card_balance_shape[0]}\\nNumber of\"\n",
    "      f\"columns on bureau data_csv: {credit_card_balance_shape[1]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Main Data Preparation\n",
    "Objective:\n",
    "In order to aggregate data to main data tables, a quick exploration is \n",
    "needed to understand which data are useful and how should each be aggregated\n",
    ". These data will also need to be split into train and test before the \n",
    "aggregation, as the main table is already split."
   ],
   "id": "c85243df6d81af0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.1. Data Cleaning\n",
    "Objective: Initial brief data cleaning to check what data is usable for \n",
    "further analysis."
   ],
   "id": "7f0680d2300e8a91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Duplicates**",
   "id": "231ed2b59a0e0fa7"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "application_test.duplicated().any()",
   "id": "d26142f38f112b5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "application_train.duplicated().any()",
   "id": "fe3a5cb5c52ce963",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "sk_id_curr_train = application_train['SK_ID_CURR']\n",
    "sk_id_curr_test = application_test['SK_ID_CURR']\n",
    "\n",
    "duplicate_ids = set(sk_id_curr_train).intersection(set(sk_id_curr_test))\n",
    "duplicate_ids"
   ],
   "id": "99b72e43d53dc9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "No duplicates, so each table has unique loan ID, SK_ID_CURR. Meaning \n",
    "SK_ID_CURR from application tables will help distinguish loans for training \n",
    "and loans for test."
   ],
   "id": "952506af98f6cd04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Check for Missing Values** -<br>\n",
    "There is a possibility of abandoning certain tables if they contained too many \n",
    "missing values."
   ],
   "id": "b7fd614ac4203cb5"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "dataframes = {\n",
    "      'application_train': application_train, \n",
    "      'application_test': application_test, \n",
    "      'bureau': bureau,\n",
    "      'bureau_balance': bureau_balance,\n",
    "      'credit_card_balance': credit_card_balance, \n",
    "      'installments_payments': installments_payments,\n",
    "      'POS_CASH_balance': POS_CASH_balance, \n",
    "      'previous_application': previous_application\n",
    "}\n",
    "\n",
    "for df_name, df in dataframes.items():\n",
    "      print(df_name)\n",
    "      display(missing_values(df).sort_values(ascending=False, by='Missing '\n",
    "                                                                 'Values'))"
   ],
   "id": "25b517b1d709d5cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There are some features with large amount of missing values. However, no one\n",
    " table has too many missing values where it needs to be discarded as a whole\n",
    " .<br>\n",
    " For modeling, tree based and non tree based models like logistic \n",
    " regression, naive bayes, random forest, XGBoost, and LightGBM will be \n",
    " tested. Non tree based models require complete data, while imputation and \n",
    " feature removal will still be considered when >40% of missing data for tree \n",
    " based models."
   ],
   "id": "770cf12b6993107"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.2. Dataframes and Keys",
   "id": "11e581bbcb493fed"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "keys_to_check = ['SK_ID_CURR', 'SK_ID_PREV', 'SK_ID_BUREAU']\n",
    "results = []\n",
    "\n",
    "for table_name, df in dataframes.items():\n",
    "    row = {'Table': table_name, 'Total_Rows': len(df)}\n",
    "    \n",
    "    for key in keys_to_check:\n",
    "        row[key] = df[key].nunique() if key in df.columns else None\n",
    "    results.append(row)\n",
    "\n",
    "key_counts_df = pd.DataFrame(results)\n",
    "key_counts_df"
   ],
   "id": "490202d0c146e1bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Total SK_ID_CURR, loan IDs : 356255<br>\n",
    "Total SK_ID_BUREAU, bureau loan IDs : 1716428, from 305811 loan IDs. \n",
    "About 14.16% of SK_ID_CURR do not have bureau info.<br>\n",
    "Total SK_ID_PREV, previous loan IDs : 1670214, from 338857 loan IDs\n",
    ". About 4.88% of SK_ID_CURR do not have previous loan info.<br><br>\n",
    "    \n",
    "**Tables with unique keys:**<br>\n",
    "    1. application_train: SK_ID_CURR<br>\n",
    "    2. application_test: SK_ID_CURR <br>\n",
    "    3. bureau: SK_ID_BUREAU. <i>Each SK_ID_CURR can correspond to \n",
    "    several SK_ID_BUREAU, and not all SK_ID_CURR are present in this \n",
    "    dataframe.</i><br>\n",
    "    4. previous_application: SK_ID_PREV. <i>Each SK_ID_CURR can correspond to \n",
    "    several SK_ID_PREV, and not all SK_ID_CURR are present in this \n",
    "    dataframe.</i><br><br>\n",
    "    \n",
    "**Tables without unique keys:**<br>\n",
    "    1. bureau_balance: SK_ID_BUREAU corresponds to several rows. Not all \n",
    "    SK_ID_BUREAU are present in this dataframe.<br>\n",
    "    2. credit_card_balance: SK_ID_PREV corresponds to several rows. Not all \n",
    "    SK_ID_PREV are present in this dataframe.<br>\n",
    "    3. installments_payments: SK_ID_PREV corresponds to several rows. Not \n",
    "    all SK_ID_PREV are present in this dataframe.<br>\n",
    "    4. POS_CASH_balance: SK_ID_PREV corresponds to several rows. Not all \n",
    "    SK_ID_PREV are present in this dataframe.<br><br>"
   ],
   "id": "1c9a5256f8edb562"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.3. Train Test Split\n",
    "Objective: \n",
    "Data split into train/test to avoid data leakage. "
   ],
   "id": "be121267e84f1753"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The tables will be first split before any analysis to minimize data \n",
    "leakage. <br>\n",
    "*bureau_balance* does not have SK_ID_CURR, so it cannot be directly split into\n",
    " train/test by that key. However, rows are identified by SK_ID_BUREAU. This \n",
    " method can also be used for *credit_card_balance*, *installments_payment*, \n",
    " and *POS_CASH_balance*.<br>"
   ],
   "id": "aab7dbcb9fab9121"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Split All Data into Train Test files -** All *test* files generated here are \n",
    "only used for kaggle submission"
   ],
   "id": "4832ad2d719a24f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split bureau by SK_ID_CURR",
   "id": "3eab58a72e20d8f2"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "bureau_train = bureau[bureau['SK_ID_CURR'].isin(sk_id_curr_train)]\n",
    "bureau_test = bureau[bureau['SK_ID_CURR'].isin(sk_id_curr_test)]\n",
    "duplicate_curr = (set(bureau_train['SK_ID_CURR'])\n",
    "                  .intersection(set(bureau_test['SK_ID_CURR'])))\n",
    "duplicate_bur = (set(bureau_train['SK_ID_BUREAU'])\n",
    "                 .intersection(set(bureau_test['SK_ID_BUREAU'])))\n",
    "print(f\"duplicated SK_ID_CURR in bureau \"\n",
    "      f\"train test tables: {duplicate_curr}\"\n",
    "      f\"\\nduplicated SK_ID_BUREAU in bureau train test tables: \"\n",
    "      f\"{duplicate_bur}\")"
   ],
   "id": "d2a1724e25fb4c28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split bureau_balance by SK_ID_BUREAU",
   "id": "4bdac08c40ae7b6b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "sk_id_bureau_train = bureau_train['SK_ID_BUREAU'].unique()\n",
    "sk_id_bureau_test = bureau_test['SK_ID_BUREAU'].unique()\n",
    "\n",
    "bureau_balance_train = bureau_balance[bureau_balance['SK_ID_BUREAU'].isin\n",
    "(sk_id_bureau_train)]\n",
    "bureau_balance_test = bureau_balance[bureau_balance['SK_ID_BUREAU'].isin\n",
    "(sk_id_bureau_test)]"
   ],
   "id": "61372dc11360e67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split previous_application by SK_ID_CURR",
   "id": "7394cf02cb0d5efc"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "previous_application_train =(\n",
    "    previous_application)[previous_application[\n",
    "    'SK_ID_CURR'].isin(sk_id_curr_train)]\n",
    "previous_application_test = (\n",
    "    previous_application)[previous_application[\n",
    "    'SK_ID_CURR'].isin(sk_id_curr_test)]\n",
    "duplicate_curr = set(previous_application_train['SK_ID_CURR']).intersection(set\n",
    "                                                         (previous_application_test['SK_ID_CURR']))\n",
    "duplicate_bur = set(previous_application_train['SK_ID_PREV']).intersection(set\n",
    "                                                         (previous_application_test['SK_ID_PREV']))\n",
    "print(f\"duplicated SK_ID_CURR in previous_application  \"\n",
    "      f\"train test tables: {duplicate_curr}\"\n",
    "      f\"\\nduplicated SK_ID_PREV in previous_application train test tables: \"\n",
    "      f\"{duplicate_bur}\")"
   ],
   "id": "77d03b5e539a6455",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Split credit_card_balance, installments_payments, POS_CASH_balance by \n",
    "SK_ID_PREV"
   ],
   "id": "1751e26fecca6a32"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "sk_id_prev_train = previous_application_train['SK_ID_PREV'].unique()\n",
    "sk_id_prev_test = previous_application_test['SK_ID_PREV'].unique()\n",
    "\n",
    "credit_card_balance_train = (\n",
    "    credit_card_balance)[credit_card_balance[('SK_ID_PREV')].isin(\n",
    "    sk_id_prev_train)]\n",
    "credit_card_balance_test = (\n",
    "    credit_card_balance)[credit_card_balance['SK_ID_PREV'].isin(\n",
    "    sk_id_prev_test)]\n",
    "\n",
    "installments_payments_train = (\n",
    "    installments_payments)[installments_payments['SK_ID_PREV'].isin(\n",
    "    sk_id_prev_train)]\n",
    "installments_payments_test = (\n",
    "    installments_payments)[installments_payments['SK_ID_PREV'].isin(\n",
    "    sk_id_prev_test)]\n",
    "\n",
    "POS_CASH_balance_train = (\n",
    "    POS_CASH_balance)[POS_CASH_balance['SK_ID_PREV'].isin(sk_id_prev_train)]\n",
    "POS_CASH_balance_test = (\n",
    "    POS_CASH_balance)[POS_CASH_balance['SK_ID_PREV'].isin(sk_id_prev_test)]"
   ],
   "id": "aac82765f6b0e28e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Save all test dataframes as .pkl files in a new folder called data_kaggle, \n",
    "as these files will only be used for kaggle submission."
   ],
   "id": "21fdc560fbe4dc23"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "test_folder = \"data_kaggle\"\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "data_pkl_folder = \"data_pkl\"\n",
    "application_test_path = os.path.join(data_pkl_folder, \"application_test.pkl\")\n",
    "application_test = pd.read_pickle(application_test_path)\n",
    "\n",
    "test_dataframes = {\n",
    "    'bureau_test': bureau_test,\n",
    "    'bureau_balance_test': bureau_balance_test,\n",
    "    'previous_application_test': previous_application_test,\n",
    "    'credit_card_balance_test': credit_card_balance_test,\n",
    "    'installments_payments_test': installments_payments_test,\n",
    "    'POS_CASH_balance_test': POS_CASH_balance_test, \n",
    "    'application_test': application_test\n",
    "}\n",
    "\n",
    "for name, df in test_dataframes.items():\n",
    "    file_path = os.path.join(test_folder, f\"{name}.pkl\")\n",
    "    df.to_pickle(file_path)\n",
    "    print(f\"Saved {name} to {file_path}\")\n",
    "\n",
    "print(\"\\nAll test DataFrames have been saved.\")"
   ],
   "id": "4ff3c6c286d7a1e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Split data into train/test for modeling**",
   "id": "70d905a1ddd36181"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Extract SK_ID_CURR for train and test\n",
    "data_train, data_test= train_test_split(\n",
    "    application_train, test_size=0.2, random_state=42, \n",
    "    stratify=application_train['TARGET']\n",
    ")\n",
    "\n",
    "sk_id_curr_train_split = application_train.loc[data_train.index, 'SK_ID_CURR']\n",
    "sk_id_curr_test_split = application_train.loc[data_test.index, 'SK_ID_CURR']\n",
    "\n",
    "# Step 2: Filter related tables\n",
    "def split_related_table(df, train_ids, test_ids, id_column):\n",
    "    train_split = df[df[id_column].isin(train_ids)]\n",
    "    test_split = df[df[id_column].isin(test_ids)]\n",
    "    return train_split, test_split\n",
    "\n",
    "related_tables = {\n",
    "    'bureau': (bureau_train, 'SK_ID_CURR'),\n",
    "    'bureau_balance': (bureau_balance_train, 'SK_ID_BUREAU'),\n",
    "    'previous_application': (previous_application_train, 'SK_ID_CURR'),\n",
    "    'credit_card_balance': (credit_card_balance_train, 'SK_ID_PREV'),\n",
    "    'installments_payments': (installments_payments_train, 'SK_ID_PREV'),\n",
    "    'POS_CASH_balance': (POS_CASH_balance_train, 'SK_ID_PREV'),\n",
    "}\n",
    "\n",
    "train_splits = {'data_train': data_train}\n",
    "test_splits = {'data_test': data_test}\n",
    "\n",
    "for table_name, (df, key_column) in related_tables.items():\n",
    "    if key_column == 'SK_ID_CURR':\n",
    "        train_split, test_split = split_related_table(df, sk_id_curr_train_split, sk_id_curr_test_split, key_column)\n",
    "    elif key_column == 'SK_ID_BUREAU':\n",
    "        sk_id_bureau_train_split = bureau_train[bureau_train['SK_ID_CURR'].isin(sk_id_curr_train_split)]['SK_ID_BUREAU']\n",
    "        sk_id_bureau_test_split = bureau_train[bureau_train['SK_ID_CURR'].isin(sk_id_curr_test_split)]['SK_ID_BUREAU']\n",
    "        train_split, test_split = split_related_table(df, sk_id_bureau_train_split, sk_id_bureau_test_split, key_column)\n",
    "    elif key_column == 'SK_ID_PREV':\n",
    "        sk_id_prev_train_split = previous_application_train[previous_application_train['SK_ID_CURR'].isin(sk_id_curr_train_split)]['SK_ID_PREV']\n",
    "        sk_id_prev_test_split = previous_application_train[previous_application_train['SK_ID_CURR'].isin(sk_id_curr_test_split)]['SK_ID_PREV']\n",
    "        train_split, test_split = split_related_table(df, sk_id_prev_train_split, sk_id_prev_test_split, key_column)\n",
    "    \n",
    "    train_splits[f\"{table_name}_train\"] = train_split\n",
    "    test_splits[f\"{table_name}_test\"] = test_split\n",
    "\n",
    "# Step 3: Save all splits to files\n",
    "split_folder = \"data_train_test_split\"\n",
    "os.makedirs(split_folder, exist_ok=True)\n",
    "\n",
    "for table_name, df in train_splits.items():\n",
    "    file_path = os.path.join(split_folder, f\"{table_name}.pkl\")\n",
    "    df.to_pickle(file_path)\n",
    "    print(f\"Saved train split for {table_name} to {file_path}\")\n",
    "\n",
    "for table_name, df in test_splits.items():\n",
    "    file_path = os.path.join(split_folder, f\"{table_name}.pkl\")\n",
    "    df.to_pickle(file_path)\n",
    "    print(f\"Saved test split for {table_name} to {file_path}\")\n",
    "\n",
    "print(\"\\nAll train-test splits have been saved.\")"
   ],
   "id": "a36f6fcc00714eb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.4. Aggregation\n",
    "Objective: Aggregate data to the main table -> application_train"
   ],
   "id": "e92dd18fcb47eb33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "# Bureau Balance Encoding\n",
    "def encode_with_onehot(df, column_to_encode, key_column, prefix):\n",
    "    # Extract the column to encode and the keys\n",
    "    keys = df[key_column]  # Preserve keys for later use\n",
    "    column = df[[column_to_encode]]  # Column to encode\n",
    "    \n",
    "    # Fit and transform the encoder\n",
    "    encoded_array = encoder.fit_transform(column)\n",
    "    \n",
    "    # Convert the encoded array to a DataFrame\n",
    "    encoded_df = pd.DataFrame(\n",
    "        encoded_array,\n",
    "        columns=[f\"{prefix}_{cat}\" for cat in encoder.categories_[0]]\n",
    "    )\n",
    "    \n",
    "    # Concatenate the keys and the encoded columns\n",
    "    encoded_df[key_column] = keys.reset_index(drop=True)  # Add keys back\n",
    "    return encoded_df"
   ],
   "id": "12f0d4529cc9d870"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "bureau = aggregate_bureau_balance(bureau, bureau_balance)\n",
    "previous_application = aggregate_previous_application(previous_application, credit_card_balance, installments_payments, POS_CASH_balance)\n",
    "previous_application.head()"
   ],
   "id": "536144ff01905c73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# for bureau_balance\n",
    "description.loc[description['Row'] == 'STATUS']"
   ],
   "id": "25040c284ad51074",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Status of Credit Bureau loan during the month (active, closed, DPD0-30, [C means closed, X means status unknown, 0 means no DPD, 1 means maximal did during month between 1-30, 2 means DPD 31-60, 5 means DPD 120+ or sold or written off ] )",
   "id": "1e79874b510e600c"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "bureau_balance['STATUS'].unique()",
   "id": "f95eb88a70338b79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "description.loc[description['Row'] == 'NAME_CONTRACT_STATUS']",
   "id": "d9670d9de663731e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "credit_card_balance['NAME_CONTRACT_STATUS'].unique()",
   "id": "1a88275de73c9bfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5910916fe72a569e",
   "metadata": {},
   "source": [
    "# Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf60b4fba79eafbf",
   "metadata": {},
   "source": [
    "1. Instead of aggregating data from other tables to the main table, maybe \n",
    "the model would benefit from predicting the probability of defaulting current \n",
    "loan based on previous bureau/loan data, which can then be aggregated to the\n",
    " main table."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "changes made to tables:<br>\n",
    "1. encode 'STATUS' and 'NAME_CONTRACT_STATUS'. one hot encoding for both"
   ],
   "id": "b56327a1748b916b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
