{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h1><center>Home Credit Risk Prediction</center></h1>\n",
    "<center> - sections 7/8 - </center>\n",
    "<center>December 2024</center>\n",
    "<center>Celine Ng</center>"
   ],
   "id": "614178cd476ca3b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. Project Introduction\n",
    "    1. Notebook Preparation\n",
    "    1. Data loading\n",
    "1. Main Data Preparation\n",
    "    1. Data cleaning\n",
    "    1. Dataframes and keys\n",
    "    1. Train Test Split\n",
    "    1. Quick EDA\n",
    "        1. Keys present in each table\n",
    "        1. Distribution\n",
    "    1. Aggregation\n",
    "1. Initial Data Cleaning\n",
    "    1. Datatypes\n",
    "    1. Missing values\n",
    "1. EDA\n",
    "    1. Original Application Table Distribution\n",
    "    1. Correlation\n",
    "    1. Statistical Inference\n",
    "1. Data Preprocessing\n",
    "1. Feature Engineering\n",
    "    1. Baseline Model\n",
    "    1. New Features\n",
    "    1. More new features\n",
    "1. Models\n",
    "    1. Pipeline\n",
    "    1. Model Selection\n",
    "    1. Test Data\n",
    "1. Final Model\n",
    "    1. Final Model\n",
    "    1. Deployment\n",
    "    1. Model Interpretation\n",
    "1. Improvements"
   ],
   "id": "d564bd4c901e5d70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:40:11.313124Z",
     "start_time": "2024-12-30T20:40:10.924818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "%pip install -r requirements.txt"
   ],
   "id": "4d440a6ef89a19c4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:40:12.617784Z",
     "start_time": "2024-12-30T20:40:11.316359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "from utils.custom_preprocessor import *\n",
    "from utils.model import *\n",
    "from utils.data_preparation import *\n",
    "from utils.get_data_merged_train import *\n",
    "\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import (OrdinalEncoder, FunctionTransformer)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from scipy.sparse import csr_matrix\n",
    "import shap"
   ],
   "id": "4a05d207024db8a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/celineng/PycharmProjects/chilng-DS.v2.5.3.4.1/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***",
   "id": "2466fada01640948"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Variables & Data from previous notebooks**",
   "id": "8dce79c94800a5f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load aggregated main table",
   "id": "975a4a9b9bc1214e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:40:13.452107Z",
     "start_time": "2024-12-30T20:40:12.672886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "folder = os.path.join(project_root, \"aggregated_data\")\n",
    "data_path = os.path.join(folder, \"data_merged.pkl\")\n",
    "data = pd.read_pickle(data_path)"
   ],
   "id": "e8030abc7cb2e86e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data",
   "id": "8d1a04db753871b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:40:17.108025Z",
     "start_time": "2024-12-30T20:40:13.468881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ml_data = data.copy()\n",
    "ml_data_sample = ml_data.sample(n=10, random_state=42)\n",
    "\n",
    "target = 'TARGET'\n",
    "binary_columns =['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE',\n",
    "                 'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21', 'EMERGENCYSTATE_MODE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CODE_GENDER']\n",
    "categorical_columns = ['NAME_CONTRACT_TYPE', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE']\n",
    "\n",
    "X, y = (ml_data.drop(columns=[target]).copy(),\n",
    "        ml_data[target].reset_index(drop=True))"
   ],
   "id": "7aad9c0877d336e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preprocessor Encoding",
   "id": "c7ede62c2200624f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:40:17.136439Z",
     "start_time": "2024-12-30T20:40:17.133842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "numerical_binary_columns = (ml_data_sample[binary_columns].select_dtypes\n",
    "                            (include='number'))\n",
    "object_binary_columns = [col for col in binary_columns if col not in numerical_binary_columns]"
   ],
   "id": "8a55a4e3463bf382",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:40:17.140383Z",
     "start_time": "2024-12-30T20:40:17.138702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocessor_encode = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('binary_encode', OrdinalEncoder(), object_binary_columns),\n",
    "        ('freq_encode', FrequencyEncoder(), categorical_columns),\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")"
   ],
   "id": "622cfd5c18c267bd",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***",
   "id": "2aa53de6991c288d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 7. Model Selection\n",
    "Objective: Compare and find the best model, hyperparameters, and decision\n",
    "threshold for final model training and deployment"
   ],
   "id": "43b02654c32d6c2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.1. Training",
   "id": "d6a5f50aa8513dc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:40:20.780323Z",
     "start_time": "2024-12-30T20:40:17.152224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ml_data = data.copy()\n",
    "ml_data_sample = ml_data.sample(n=10, random_state=42)\n",
    "target = 'TARGET'\n",
    "\n",
    "X, y = (ml_data.drop(columns=[target]).copy(),\n",
    "        ml_data[target].reset_index(drop=True))"
   ],
   "id": "21de9665e653e46c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Define Models**",
   "id": "c679caa79d80051d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-30T20:40:20.804327Z",
     "start_time": "2024-12-30T20:40:20.799514Z"
    }
   },
   "source": [
    "clf_xgb = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric = 'auc',\n",
    "    random_state=42,\n",
    "    scale_pos_weight = (y == 0).sum() / (y == 1).sum()\n",
    ")\n",
    "\n",
    "clf_lgbm = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    eval_metric = 'auc',\n",
    "    random_state=42,\n",
    "    scale_pos_weight = (y == 0).sum() / (y == 1).sum()\n",
    ")\n",
    "\n",
    "clf_rf = RandomForestClassifier(\n",
    "    n_estimators = 100,\n",
    "    class_weight = 'balanced',\n",
    "    random_state = 42,\n",
    "    n_jobs = -1\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Define Pipeline**",
   "id": "aeb6050a5af19038"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:40:20.820097Z",
     "start_time": "2024-12-30T20:40:20.817811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor_encode),\n",
    "    ('feature_creator', FeatureCreation2()),\n",
    "    ('model', clf_xgb)\n",
    "])\n",
    "\n",
    "lgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor_encode),\n",
    "    ('feature_creator', FeatureCreation2()),\n",
    "    ('model', clf_lgbm)\n",
    "])\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor_encode),\n",
    "    ('feature_creator', FeatureCreation2()),\n",
    "    ('model', clf_rf)\n",
    "])"
   ],
   "id": "691c9a87940786bf",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Constructing pipelines is a more robust and scalable method, due\n",
    "to time constraints and existent bugs, manual training will be applied for\n",
    "this project."
   ],
   "id": "62687035b57b8021"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:40:20.843448Z",
     "start_time": "2024-12-30T20:40:20.839015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_folder = os.path.join(\"..\", \"results\")\n",
    "results_file = os.path.join(results_folder, \"modes_results.pkl\")\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, \"rb\") as f:\n",
    "        results = pkl.load(f)\n",
    "    print(f\"Results loaded from {results_file}\")\n",
    "else:\n",
    "    results = {'XGB': [], 'LGBM': [], 'RF': []}\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        X_train_processed = pd.DataFrame(preprocessor_encode.fit_transform(X_train), columns=preprocessor_encode.get_feature_names_out())\n",
    "        X_test_processed = pd.DataFrame(preprocessor_encode.transform(X_test), columns=preprocessor_encode.get_feature_names_out())\n",
    "\n",
    "        fc = FeatureCreation2()\n",
    "        X_train_processed = fc.fit_transform(X_train_processed)\n",
    "        X_test_processed = fc.transform(X_test_processed)\n",
    "\n",
    "        for model_name, clf in [('XGB', clf_xgb), ('LGBM', clf_lgbm), ('RF', clf_rf)]:\n",
    "            clf.fit(X_train_processed, y_train)\n",
    "            y_pred = clf.predict_proba(X_test_processed)[:, 1]\n",
    "            auc_score = roc_auc_score(y_test, y_pred)\n",
    "            results[model_name].append(auc_score)\n",
    "\n",
    "    with open(results_file, \"wb\") as f:\n",
    "        pkl.dump(results, f)\n",
    "    print(f\"Results saved to {results_file}\")\n",
    "\n",
    "for model_name, scores in results.items():\n",
    "    print(f\"{model_name}: Mean AUC = {sum(scores)/len(scores):.4f}, Scores = {scores}\")"
   ],
   "id": "c82a2940b19c78c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results loaded from ../results/modes_results.pkl\n",
      "XGB: Mean AUC = 0.7570, Scores = [np.float64(0.7569814759028433), np.float64(0.7570831883320318), np.float64(0.7602751456748199), np.float64(0.7583395348504778), np.float64(0.752283294966421)]\n",
      "LGBM: Mean AUC = 0.7785, Scores = [np.float64(0.7771454069710615), np.float64(0.7762573421868177), np.float64(0.7822009761454211), np.float64(0.7794726531741337), np.float64(0.7774387269955776)]\n",
      "RF: Mean AUC = 0.7253, Scores = [np.float64(0.7253180788624507), np.float64(0.7218548967245626), np.float64(0.7313602385613733), np.float64(0.7252500684833187), np.float64(0.7228668706994196)]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Comparing the scores across 3 models, with stratified k fold, and the whole\n",
    "training set, it is clear that LightGBM is the best model in all folds and\n",
    "in general. The score achieved by LightGBM is close to the score achieved by\n",
    " XGBoost after hyperparameter tuning from our previous experience. However,\n",
    " we will proceed with hyperparameter tuning just to confirm the results."
   ],
   "id": "5a61312f7e4df0ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.2. Hyperparameter Tuning",
   "id": "8afc68188a406f7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cross validation is not used at this step due to memory limitation",
   "id": "75648d9c87b823c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Define hyperparameter intervals for tuning**",
   "id": "e1e2ed35441cb664"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:40:20.883403Z",
     "start_time": "2024-12-30T20:40:20.879308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'random_state': 42,\n",
    "        'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum(),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5)\n",
    "    }\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_train_processed, y_train):\n",
    "        X_t, X_v = X_train_processed.iloc[train_idx], X_train_processed.iloc[val_idx]\n",
    "        y_t, y_v = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        dtrain = xgb.DMatrix(data=X_t, label=y_t)\n",
    "        dval = xgb.DMatrix(data=X_v, label=y_v)\n",
    "\n",
    "        model = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=500,\n",
    "            evals=[(dval, 'validation')],\n",
    "            early_stopping_rounds=15,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        y_pred_proba = model.predict(dval)\n",
    "        auc_scores.append(roc_auc_score(y_v, y_pred_proba))\n",
    "\n",
    "    return sum(auc_scores) / len(auc_scores)\n",
    "\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'random_state': 42,\n",
    "        'verbosity': -1,\n",
    "        'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum(),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10)\n",
    "    }\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_train_processed, y_train):\n",
    "        X_t, X_v = X_train_processed.iloc[train_idx], X_train_processed.iloc[val_idx]\n",
    "        y_t, y_v = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_t, y_t,\n",
    "            eval_set=[(X_v, y_v)],\n",
    "            eval_metric='auc',\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=15)]\n",
    "        )\n",
    "        y_pred_proba = model.predict_proba(X_v)[:, 1]\n",
    "        auc_scores.append(roc_auc_score(y_v, y_pred_proba))\n",
    "\n",
    "    return sum(auc_scores) / len(auc_scores)\n"
   ],
   "id": "96e54027f8c0fef9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Tune and serialize best parameters**",
   "id": "3b8811556383d69d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:40:20.899140Z",
     "start_time": "2024-12-30T20:40:20.890650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hyperparameters_folder = os.path.join(\"..\", \"hyperparameters\")\n",
    "os.makedirs(hyperparameters_folder, exist_ok=True)\n",
    "studies = {}\n",
    "\n",
    "for model_name, objective in zip([\"XGB\", \"LGBM\"], [objective_xgb,\n",
    "                                                         objective_lgbm]):\n",
    "    study_file = os.path.join(hyperparameters_folder, f\"{model_name.lower()}_study.pkl\")\n",
    "\n",
    "    if os.path.exists(study_file):\n",
    "        try:\n",
    "            with open(study_file, \"rb\") as f:\n",
    "                studies[model_name] = pkl.load(f)\n",
    "            print(f\"Loaded existing study for {model_name} from: {study_file}\")\n",
    "        except (pkl.UnpicklingError, EOFError, FileNotFoundError) as e:\n",
    "            print(f\"Failed to load study for {model_name}: {e}\")\n",
    "            studies[model_name] = None\n",
    "    else:\n",
    "        print(f\"No study file found for {model_name}. Starting new study...\")\n",
    "        studies[model_name] = optuna.create_study(direction='maximize')\n",
    "        studies[model_name].optimize(locals()[f\"objective_{model_name.lower()}\"], n_trials=50)\n",
    "\n",
    "        with open(study_file, \"wb\") as f:\n",
    "            pkl.dump(studies[model_name], f)\n",
    "        print(f\"Saved new study for {model_name} to: {study_file}\")\n",
    "\n",
    "for model_name, study in studies.items():\n",
    "    if study is not None:\n",
    "        print(f\"{model_name} Best Parameters: {study.best_params}\")\n",
    "        print(f\"{model_name} Best ROC AUC: {study.best_value:.4f}\")\n",
    "    else:\n",
    "        print(f\"No valid study for {model_name}.\")"
   ],
   "id": "e9c6bae860c50bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing study for XGB from: ../hyperparameters/xgb_study.pkl\n",
      "Loaded existing study for LGBM from: ../hyperparameters/lgbm_study.pkl\n",
      "XGB Best Parameters: {'learning_rate': 0.06924801251538602, 'max_depth': 3, 'min_child_weight': 8.40069684967606, 'subsample': 0.6865465116290375, 'colsample_bytree': 0.8358949053127434, 'gamma': 1.545494502279399}\n",
      "XGB Best ROC AUC: 0.7794\n",
      "LGBM Best Parameters: {'learning_rate': 0.0012867544702033604, 'num_leaves': 99, 'min_child_samples': 24, 'subsample': 0.6357962739326517, 'colsample_bytree': 0.6249619518784197, 'reg_alpha': 9.950623659513164, 'reg_lambda': 1.7731285679565518}\n",
      "LGBM Best ROC AUC: 0.7492\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "LightGBM did way better with default settings. There must be a range which\n",
    "optimizes AUC score. For now, we use the model in its default settings"
   ],
   "id": "89ce4e9e9e81887b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7.3. Ensemble Model\n",
    "Objective: By combining predictions of several models, we hope to improve\n",
    "the overall prediction by leveraging varying strengths of different models."
   ],
   "id": "2f71e542ee18947c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Weighted Averaging**",
   "id": "335a95c12ac3aa63"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "XGBoost (after hyperparameter tuning) and LightGBM (default) have similar\n",
    " AUC score, and RandomForest is the weakest. Weights will be assigned by\n",
    " performance to ensure the ensemble benefits more from the stronger models\n",
    " while minimizing the influence of Random Forest."
   ],
   "id": "f50e8288d3a6854a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:40:20.924377Z",
     "start_time": "2024-12-30T20:40:20.920060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_folder = os.path.join(\"..\", \"results\")\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "ensemble_results_file = os.path.join(results_folder, \"ensemble_results.pkl\")\n",
    "\n",
    "if os.path.exists(ensemble_results_file):\n",
    "    with open(ensemble_results_file, \"rb\") as f:\n",
    "        results = pkl.load(f)\n",
    "    print(f\"Results loaded from {ensemble_results_file}\")\n",
    "else:\n",
    "    results = {'Ensemble': []}\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Load the best XGBoost parameters\n",
    "    xgb_params = studies['XGB'].best_params if 'XGB' in studies and studies['XGB'] else {}\n",
    "\n",
    "    xgb_params.update({\n",
    "        'objective': 'binary:logistic',\n",
    "        'random_state': 42,\n",
    "        'scale_pos_weight': (y == 0).sum() / (y == 1).sum()\n",
    "    })\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Preprocessing\n",
    "        X_train_processed = pd.DataFrame(preprocessor_encode.fit_transform(X_train),\n",
    "                                         columns=preprocessor_encode.get_feature_names_out())\n",
    "        X_test_processed = pd.DataFrame(preprocessor_encode.transform(X_test),\n",
    "                                        columns=preprocessor_encode.get_feature_names_out())\n",
    "\n",
    "        # Feature creation\n",
    "        fc = FeatureCreation2()\n",
    "        X_train_processed = fc.fit_transform(X_train_processed)\n",
    "        X_test_processed = fc.transform(X_test_processed)\n",
    "\n",
    "        # Initialize models\n",
    "        clf_xgb = xgb.XGBClassifier(**xgb_params)\n",
    "        clf_lgbm = lgb.LGBMClassifier(\n",
    "            objective='binary',\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            scale_pos_weight=(y == 0).sum() / (y == 1).sum()\n",
    "        )\n",
    "        clf_rf = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            class_weight='balanced',\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Fit each base model\n",
    "        clf_xgb.fit(X_train_processed, y_train)\n",
    "        clf_lgbm.fit(X_train_processed, y_train)\n",
    "        clf_rf.fit(X_train_processed, y_train)\n",
    "\n",
    "        # Predict using weighted averaging ensemble\n",
    "        y_pred_ensemble = (\n",
    "                0.4 * clf_xgb.predict_proba(X_test_processed)[:, 1] +\n",
    "                0.4 * clf_lgbm.predict_proba(X_test_processed)[:, 1] +\n",
    "                0.2 * clf_rf.predict_proba(X_test_processed)[:, 1]\n",
    "        )\n",
    "\n",
    "        auc_score = roc_auc_score(y_test, y_pred_ensemble)\n",
    "        results['Ensemble'].append(auc_score)\n",
    "\n",
    "    # Serialize the ensemble results\n",
    "    with open(ensemble_results_file, \"wb\") as f:\n",
    "        pkl.dump(results, f)\n",
    "    print(f\"Ensemble results saved to {ensemble_results_file}\")\n",
    "\n",
    "print(f\"Ensemble: Mean AUC = {sum(results['Ensemble']) / len(results['Ensemble']):.4f}, Scores = {results['Ensemble']}\")"
   ],
   "id": "5b00caf5666c0f70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results loaded from ../results/ensemble_results.pkl\n",
      "Ensemble: Mean AUC = 0.7761, Scores = [np.float64(0.7738403625288581), np.float64(0.7740119538961543), np.float64(0.7804344984869769), np.float64(0.777711240840697), np.float64(0.7745577184401893)]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Voting - Soft**",
   "id": "43e183cf9a35d4d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:40:20.949236Z",
     "start_time": "2024-12-30T20:40:20.944814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_folder = os.path.join(\"..\", \"results\")\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "ensemble_results_file = os.path.join(results_folder, \"voting_results.pkl\")\n",
    "\n",
    "if os.path.exists(ensemble_results_file):\n",
    "    with open(ensemble_results_file, \"rb\") as f:\n",
    "        results = pkl.load(f)\n",
    "    print(f\"Results loaded from {ensemble_results_file}\")\n",
    "else:\n",
    "    results = {'Ensemble': []}\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Load the best XGBoost parameters\n",
    "    xgb_params = studies['XGB'].best_params if 'XGB' in studies and studies['XGB'] else {}\n",
    "\n",
    "    xgb_params.update({\n",
    "        'objective': 'binary:logistic',\n",
    "        'random_state': 42,\n",
    "        'scale_pos_weight': (y == 0).sum() / (y == 1).sum()\n",
    "    })\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X, y):\n",
    "        # Split data\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Preprocessing\n",
    "        X_train_processed = pd.DataFrame(preprocessor_encode.fit_transform(X_train),\n",
    "                                         columns=preprocessor_encode.get_feature_names_out())\n",
    "        X_test_processed = pd.DataFrame(preprocessor_encode.transform(X_test),\n",
    "                                        columns=preprocessor_encode.get_feature_names_out())\n",
    "\n",
    "        # Feature creation\n",
    "        fc = FeatureCreation2()\n",
    "        X_train_processed = fc.fit_transform(X_train_processed)\n",
    "        X_test_processed = fc.transform(X_test_processed)\n",
    "\n",
    "        # Initialize models with parameters\n",
    "        clf_xgb = xgb.XGBClassifier(**xgb_params)\n",
    "        clf_lgbm = lgb.LGBMClassifier(\n",
    "            objective='binary',\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            scale_pos_weight=(y == 0).sum() / (y == 1).sum()\n",
    "        )\n",
    "        clf_rf = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            class_weight='balanced',\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Create Voting Classifier\n",
    "        ensemble = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('xgb', clf_xgb),\n",
    "                ('lgbm', clf_lgbm),\n",
    "                ('rf', clf_rf)\n",
    "            ],\n",
    "            voting='soft'  # Use probabilities for combining predictions\n",
    "        )\n",
    "\n",
    "        # Fit ensemble model\n",
    "        ensemble.fit(X_train_processed, y_train)\n",
    "\n",
    "        y_pred_ensemble = ensemble.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "        auc_score = roc_auc_score(y_test, y_pred_ensemble)\n",
    "        results['Ensemble'].append(auc_score)\n",
    "\n",
    "    # Serialize the ensemble results\n",
    "    with open(ensemble_results_file, \"wb\") as f:\n",
    "        pkl.dump(results, f)\n",
    "    print(f\"Ensemble results saved to {ensemble_results_file}\")\n",
    "\n",
    "print(f\"Ensemble: Mean AUC = {sum(results['Ensemble']) / len(results['Ensemble']):.4f}, Scores = {results['Ensemble']}\")"
   ],
   "id": "406e109b6956ae1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results loaded from ../results/voting_results.pkl\n",
      "Ensemble: Mean AUC = 0.7759, Scores = [np.float64(0.7736559520445907), np.float64(0.773745368586072), np.float64(0.7804179054397808), np.float64(0.7775234641784984), np.float64(0.7741448535913146)]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since both ensemble models do not provide significant AUC score\n",
    "improvements, these are not worth pursuing.<br>\n",
    "Between XGBoost and LightGBM, as XGBoost does have a slighter higher score,\n",
    "XGBoost is the best choice here."
   ],
   "id": "f4db5c0f6fec4328"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7.4. Decision Threshold",
   "id": "365fb79656ca3c"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-30T22:33:04.002338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_folder = os.path.join(\"..\", \"results\")\n",
    "results_file = os.path.join(results_folder, \"decision_threshold.pkl\")\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, \"rb\") as f:\n",
    "        results = pkl.load(f)\n",
    "    print(f\"Results loaded from {results_file}\")\n",
    "else:\n",
    "    results = {'Mean AUC': [], 'Best Threshold': []}\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    xgb_params = studies['XGB'].best_params if 'XGB' in studies and studies['XGB'] else {}\n",
    "\n",
    "    xgb_params.update({\n",
    "        'objective': 'binary:logistic',\n",
    "        'random_state': 42,\n",
    "        'scale_pos_weight': (y == 0).sum() / (y == 1).sum()\n",
    "    })\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        X_train_processed = pd.DataFrame(preprocessor_encode.fit_transform(X_train), columns=preprocessor_encode.get_feature_names_out())\n",
    "        X_test_processed = pd.DataFrame(preprocessor_encode.transform(X_test), columns=preprocessor_encode.get_feature_names_out())\n",
    "\n",
    "        fc = FeatureCreation2()\n",
    "        X_train_processed = fc.fit_transform(X_train_processed)\n",
    "        X_test_processed = fc.transform(X_test_processed)\n",
    "\n",
    "        clf_xgb = xgb.XGBClassifier(**xgb_params)\n",
    "        clf_xgb.fit(X_train_processed, y_train)\n",
    "\n",
    "        # Predict probabilities\n",
    "        y_pred = clf_xgb.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "        # Compute AUC\n",
    "        auc_score = roc_auc_score(y_test, y_pred)\n",
    "        results['Mean AUC'].append(auc_score)\n",
    "\n",
    "        # Find the best threshold\n",
    "        thresholds = np.linspace(0, 1, 100)\n",
    "        best_threshold = 0.5\n",
    "        best_f1 = 0\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            y_pred_class = (y_pred >= threshold).astype(int)\n",
    "            f1 = f1_score(y_test, y_pred_class)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "\n",
    "        results['Best Threshold'].append(best_threshold)\n",
    "        print(f\"Fold AUC: {auc_score:.4f}, Best Threshold: {best_threshold}, Best F1 Score: {best_f1:.4f}\")\n",
    "\n",
    "    # Save results\n",
    "    with open(results_file, \"wb\") as f:\n",
    "        pkl.dump(results, f)\n",
    "    print(f\"Results saved to {results_file}\")\n",
    "\n",
    "# Summarize overall metrics\n",
    "mean_auc = sum(results['Mean AUC']) / len(results['Mean AUC'])\n",
    "mean_best_threshold = sum(results['Best Threshold']) / len(results['Best Threshold'])\n",
    "\n",
    "print(f\"Overall Mean AUC: {mean_auc:.4f}\")\n",
    "print(f\"Overall Best Threshold: {mean_best_threshold:.4f}\")\n"
   ],
   "id": "dbcf121e9fc80f72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 8. Final Model",
   "id": "9cb38080ce608360"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load test data",
   "id": "73618bc4ffe0958"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:40:21.297142Z",
     "start_time": "2024-12-30T20:40:20.967612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "folder = os.path.join(project_root, \"data_train_test_split\")\n",
    "folder_2 = os.path.join(folder, \"test_split\")\n",
    "\n",
    "pkl_files = [f for f in os.listdir(folder_2) if f.endswith('.pkl')]\n",
    "\n",
    "test_splits = {}\n",
    "for file_name in pkl_files:\n",
    "    file_path = os.path.join(folder_2, file_name)\n",
    "    test_splits[file_name.split('.')[0]] = pd.read_pickle(file_path)"
   ],
   "id": "9018c9bcf89299e8",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Encode and Aggregate complementary tables to main table for test data",
   "id": "722d0bb3af952049"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:45:48.345763Z",
     "start_time": "2024-12-30T20:45:42.584321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_folder = os.path.join(\"..\", \"aggregated_data\")\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "aggregated_file = os.path.join(results_folder, \"data_merged_test.pkl\")\n",
    "\n",
    "if os.path.exists(aggregated_file):\n",
    "    with open(aggregated_file, \"rb\") as f:\n",
    "        data_merged_test = pkl.load(f)\n",
    "    print(f\"Aggregated data loaded from {aggregated_file}\")\n",
    "else:\n",
    "    # Perform encoding and aggregation\n",
    "    print(\"Aggregated file not found. Performing encoding and aggregation...\")\n",
    "\n",
    "    # Encode bureau_balance, credit_card_balance, POS_CASH, installments_payments\n",
    "    bureau_balance_encoded = encode_object_columns_with_ordinal(\n",
    "        test_splits['bureau_balance_test']\n",
    "    )\n",
    "    credit_card_balance_encoded = encode_object_columns_with_ordinal(\n",
    "        test_splits['credit_card_balance_test']\n",
    "    )\n",
    "    POS_CASH_balance_encoded = encode_object_columns_with_ordinal(\n",
    "        test_splits['POS_CASH_balance_test']\n",
    "    )\n",
    "    previous_application_encoded = encode_object_columns_with_ordinal(\n",
    "        test_splits['previous_application_test']\n",
    "    )\n",
    "\n",
    "    # Aggregate to bureau and previous_application\n",
    "    bureau_aggregator = BureauBalanceAggregator(agg_funcs=['mean', 'sum', 'min', 'max', 'count'])\n",
    "    bureau_transformed = bureau_aggregator.transform(\n",
    "        (test_splits['bureau_test'], bureau_balance_encoded)\n",
    "    )\n",
    "\n",
    "    previous_application_aggregator = PreviousApplicationAggregator(agg_funcs=['mean', 'sum', 'min', 'max', 'count'])\n",
    "    previous_application_transformed = previous_application_aggregator.transform(\n",
    "        (previous_application_encoded, credit_card_balance_encoded,\n",
    "         test_splits['installments_payments_test'], POS_CASH_balance_encoded)\n",
    "    )\n",
    "\n",
    "    # Aggregate bureau and previous_application to main table\n",
    "    data_merged_test = get_data_merged_test(bureau_transformed,\n",
    "                                             previous_application_transformed,\n",
    "                                             test_splits)\n",
    "\n",
    "    with open(aggregated_file, \"wb\") as f:\n",
    "        pkl.dump(data_merged_test, f)\n",
    "    print(f\"Aggregated data saved to {aggregated_file}\")\n"
   ],
   "id": "5f54d8e53140d56b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated file not found. Performing encoding and aggregation...\n",
      "\n",
      "[2024-12-30 20:45:45.375132] === Starting data merge test process ===\n",
      "[2024-12-30 20:45:45.375265] Processing bureau data...\n",
      "\n",
      "[2024-12-30 20:45:45.375285] === Starting bureau processing ===\n",
      "[2024-12-30 20:45:45.375288] Initial bureau shape: (294026, 27)\n",
      "[2024-12-30 20:45:45.375299] Encoding object columns in bureau data...\n",
      "[2024-12-30 20:45:45.375311] Starting ordinal encoding...\n",
      "[2024-12-30 20:45:45.392601] Found 3 object columns to encode\n",
      "[2024-12-30 20:45:45.392620] Processing object columns...\n",
      "[2024-12-30 20:45:45.400759] Initializing OrdinalEncoder...\n",
      "[2024-12-30 20:45:45.400775] Performing encoding transformation...\n",
      "[2024-12-30 20:45:45.464508] Creating DataFrame with encoded values...\n",
      "[2024-12-30 20:45:45.464623] Concatenating encoded and non-encoded columns...\n",
      "[2024-12-30 20:45:45.471195] Ordinal encoding completed successfully\n",
      "[2024-12-30 20:45:45.473667] Bureau encoding completed\n",
      "[2024-12-30 20:45:45.473673] Calculating bureau counts...\n",
      "[2024-12-30 20:45:45.482125] Bureau counts shape: (52727, 2)\n",
      "[2024-12-30 20:45:45.482138] Cleaning bureau data...\n",
      "[2024-12-30 20:45:45.486981] Cleaned bureau shape: (294026, 26)\n",
      "[2024-12-30 20:45:45.486993] Identifying numeric columns...\n",
      "[2024-12-30 20:45:45.497640] Found 25 numeric columns for aggregation\n",
      "[2024-12-30 20:45:45.497649] Performing aggregation...\n",
      "[2024-12-30 20:45:45.675257] Aggregated bureau shape: (52727, 126)\n",
      "[2024-12-30 20:45:45.675315] Renaming columns...\n",
      "[2024-12-30 20:45:45.675402] Performing final bureau merge...\n",
      "[2024-12-30 20:45:45.675419] About to run merge...\n",
      "[2024-12-30 20:45:45.680833] Bureau processing completed. Final shape: (52727, 128)\n",
      "\n",
      "[2024-12-30 20:45:45.683962] Processing previous application data...\n",
      "\n",
      "[2024-12-30 20:45:45.683983] === Starting previous application processing ===\n",
      "[2024-12-30 20:45:45.683987] Initial previous application shape: (282363, 202)\n",
      "[2024-12-30 20:45:45.683991] Encoding object columns in previous application data...\n",
      "[2024-12-30 20:45:45.683997] Starting ordinal encoding...\n",
      "[2024-12-30 20:45:45.858691] Found 0 object columns to encode\n",
      "[2024-12-30 20:45:45.858716] Ordinal encoding completed successfully\n",
      "[2024-12-30 20:45:45.858722] Previous application encoding completed\n",
      "[2024-12-30 20:45:45.858725] Calculating previous application counts...\n",
      "[2024-12-30 20:45:45.867443] Previous counts shape: (58223, 2)\n",
      "[2024-12-30 20:45:45.867457] Cleaning previous application data...\n",
      "[2024-12-30 20:45:45.922880] Cleaned previous application shape: (282363, 201)\n",
      "[2024-12-30 20:45:45.922942] Identifying numeric columns...\n",
      "[2024-12-30 20:45:45.984227] Found 200 numeric columns for aggregation\n",
      "[2024-12-30 20:45:45.984261] Performing aggregation...\n",
      "[2024-12-30 20:45:47.478654] Aggregated previous application shape: (58223, 1001)\n",
      "[2024-12-30 20:45:47.478733] Renaming columns...\n",
      "[2024-12-30 20:45:47.479147] Performing final previous application merge...\n",
      "[2024-12-30 20:45:47.569425] Previous application processing completed. Final shape: (58223, 1003)\n",
      "\n",
      "[2024-12-30 20:45:47.582562] Performing final merge with main table...\n",
      "[2024-12-30 20:45:47.582587] Merging bureau data with main table...\n",
      "[2024-12-30 20:45:47.626991] Shape after bureau merge: (61503, 249)\n",
      "[2024-12-30 20:45:47.627011] Merging previous application data...\n",
      "[2024-12-30 20:45:48.096214] Final merged shape: (61503, 1251)\n",
      "\n",
      "SK_ID_BUREAU_count missing values: 0 \n",
      "\n",
      "SK_ID_PREV_count missing values: 0\n",
      "[2024-12-30 20:45:48.099450] Data merge test process completed successfully!\n",
      "Aggregated data saved to ../aggregated_data/data_merged_test.pkl\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split test data into data and target",
   "id": "906657b2a2a53f0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T20:45:58.340470Z",
     "start_time": "2024-12-30T20:45:57.837456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_test = data_merged_test.copy()\n",
    "\n",
    "X_test, y_test = (data_test.drop(columns=[target]).copy(),\n",
    "                  data_test[target].reset_index(drop=True))"
   ],
   "id": "8c8175132ee2f237",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Fit and Train",
   "id": "4a57fdfb33569dab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cac73bbc7a2b1096"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T21:54:02.445554Z",
     "start_time": "2024-12-30T21:53:39.487544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Define results folder and file\n",
    "results_folder = os.path.join(\"..\", \"results\")\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "results_file = os.path.join(results_folder, \"final_results.pkl\")\n",
    "\n",
    "# Load existing results if available\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, \"rb\") as f:\n",
    "        results = pkl.load(f)\n",
    "    print(f\"Results loaded from {results_file}\")\n",
    "else:\n",
    "    results = []\n",
    "\n",
    "    # Load the best XGBoost parameters\n",
    "    xgb_params = studies['XGB'].best_params if 'XGB' in studies and studies['XGB'] else {}\n",
    "\n",
    "    # Update parameters for training\n",
    "    xgb_params.update({\n",
    "        'objective': 'binary:logistic',\n",
    "        'random_state': 42,\n",
    "        'scale_pos_weight': (y == 0).sum() / (y == 1).sum()\n",
    "    })\n",
    "\n",
    "    # Preprocessing\n",
    "    X_train_processed = pd.DataFrame(preprocessor_encode.fit_transform(X),\n",
    "                                     columns=preprocessor_encode.get_feature_names_out())\n",
    "    X_test_processed = pd.DataFrame(preprocessor_encode.transform(X_test),\n",
    "                                    columns=preprocessor_encode.get_feature_names_out())\n",
    "\n",
    "    # Feature Creation\n",
    "    fc = FeatureCreation2()\n",
    "    X_train_processed = fc.fit_transform(X_train_processed)\n",
    "    X_test_processed = fc.transform(X_test_processed)\n",
    "\n",
    "    # Train the model on the train set\n",
    "    clf_xgb = xgb.XGBClassifier(**xgb_params)\n",
    "    clf_xgb.fit(X_train_processed, y)\n",
    "\n",
    "    # Predict and calculate AUC on the test set\n",
    "    y_pred = clf_xgb.predict_proba(X_test_processed)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_pred)\n",
    "    results.append(auc_score)\n",
    "\n",
    "    # Save results\n",
    "    with open(results_file, \"wb\") as f:\n",
    "        pkl.dump(results, f)\n",
    "    print(f\"Results saved to {results_file}\")\n",
    "\n",
    "# Display the results\n",
    "print(f\"Final Model Mean AUC = {sum(results) / len(results):.4f}, Scores = {results}\")\n"
   ],
   "id": "a2ff6d6207f2d811",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ../results/final_results.pkl\n",
      "Final Model Mean AUC = 0.7672, Scores = [np.float64(0.7672492423440078)]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Train pipeline\n",
    "preprocessing_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save pipeline\n",
    "import joblib\n",
    "joblib.dump(preprocessing_pipeline, 'credit_risk_pipeline.joblib')\n",
    "\n",
    "# Load and predict (in production)\n",
    "loaded_pipeline = joblib.load('credit_risk_pipeline.joblib')\n",
    "predictions = loaded_pipeline.predict(new_data)"
   ],
   "id": "d10792c5102d586c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hyperparameter tuning with stratified k fold",
   "id": "c06da70494e1519b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "# Define the Optuna objective function with Stratified K-Fold\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.8),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.8),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'scale_pos_weight': scale_pos_weight,  # Use precomputed weight\n",
    "        'random_state': 42,\n",
    "    }\n",
    "\n",
    "    # Update clf_xgb parameters with sampled hyperparameters\n",
    "    clf_xgb.set_params(**params)\n",
    "\n",
    "    # Stratified K-Fold Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_train_tf, y_train):\n",
    "        # Split the data into train and validation sets\n",
    "        X_train_fold, X_val_fold = X_train_tf.iloc[train_idx], X_train_tf.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        # Train the model\n",
    "        clf_xgb.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            early_stopping_rounds=10,  # Use early stopping\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # Predict probabilities for validation fold\n",
    "        y_prob = clf_xgb.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "        # Compute AUC for the fold\n",
    "        auc_scores.append(roc_auc_score(y_val_fold, y_prob))\n",
    "\n",
    "    # Return the mean AUC across all folds\n",
    "    return np.mean(auc_scores)\n",
    "\n",
    "# Create and run the Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best ROC AUC:\", study.best_value)"
   ],
   "id": "ef3fa0b9ae99db2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. preprocessing steps ={\n",
    "feature creation (BUREAU_ID, PREV_ID, other domain knowledge based features)\n",
    "encoding (columns from main table contain object dtypes)\n",
    "}\n",
    "2. feature selection if too slow -> model train  -> hyperparameter (select\n",
    "depth\n",
    " to remove useless features) -> train/test cross validation, metrics\n",
    "3. repeat step 2 for all models used (random forest with class weight,\n",
    "imblearn random forest, xgboost with pos scale weight....?)"
   ],
   "id": "98c3eeef5251136c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor_encode),\n",
    "    ('feature_creator', FeatureCreation2()),\n",
    "    ('model', clf_xgb)\n",
    "])\n",
    "\n",
    "# Train pipeline\n",
    "preprocessing_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save pipeline\n",
    "import joblib\n",
    "joblib.dump(preprocessing_pipeline, 'credit_risk_pipeline.joblib')\n",
    "\n",
    "# Load and predict (in production)\n",
    "loaded_pipeline = joblib.load('credit_risk_pipeline.joblib')\n",
    "predictions = loaded_pipeline.predict(new_data)"
   ],
   "id": "b5db41541f4eb904"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Improvements",
   "id": "8ba877522b3ac674"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Ensemble models are providing very similar results to single models,\n",
    "possibly both LGB and XGBoost have similar predictions. Adding a Logistic\n",
    "Regression (simpler patterns) or kNN (local patterns) can help diversify\n",
    "predictions."
   ],
   "id": "c210202eb63889ef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
